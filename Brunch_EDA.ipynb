{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA for Brunch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jihoo-kim/BrunchRec/blob/master/BrunchRec_v9.0.4.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석 목적\n",
    "- 다양한 미디어 매체를 통해 콘텐츠의 가치가 나날이 높아지면서 양질의 콘텐츠가 늘어나게 되었고 이에 따라 손쉽고 편하게 나에게 맞는 콘텐츠를 추천받길 원하는 사용자가 늘어나고 있음\n",
    "- 추천을 통해 글이 가치 있게 읽히는 경험은 창작자, 독자 모두에게 매우 중요한 일\n",
    "- 브런치에 담긴 글을 원하는 독자가 충분히 감상할 수 있도록 추천하는 것이 목적\n",
    "- 사용자의 과거 활동 정보를 기반으로 취향을 분석한 후 모델링과정을 통해 미래 소비 결과를 예측해 보다 정밀하게 사용자 개개인이 좋아할 만한 글을 예측하는 것이 목표\n",
    "- 데이터의 구성 :\n",
    "    - 콘텐츠 : metadata.json, contents, magazine.json\n",
    "    - 작가/독자 정보 : users.json\n",
    "    - 행태 정보 : read.tar\n",
    "    - (예측) predict.tar \n",
    "    \n",
    "### 용어정리\n",
    "- 브런치 : 브런치 (brunch.co.kr)은 다음카카오에서 새롭게 만든 서비스로 트위터 창업자인 에반 윌리엄스가 만든 미디엄(medium)를 벤치마킹한 서비스.\n",
    "한국판 미디엄으로 글을 쓰고 보는데 가장 집중도가 높도록 심플함을 강조한 블로그 플랫폼.\n",
    "브런치는 네이버 포스트와 유사하게 작가와 구독자의 개념으로 구분되어있지만 네이버 포스트가 작가를 발굴하여 나간다는 개념인 반면 브런치는 작가 혹은 작가에 견줄만한 이력을 갖춘 사람들을 선별하여 승인 절차를 거쳐야만 가입이 가능하다는 점에서 차이가 있음.\n",
    "네이버 포스터가 했던 것처럼 브런치에서 쓰여질 양질의 컨텐츠는 카카오톡 검색 서비스 그리고 카카오스토리 해쉬태그 검색을 비롯한 다음의 검색에도 반영될것으로 보임\n",
    "- 매거진 : 미디엄의 컬랙션(Collection)과 유사한 기능인 브런치의 매거진이 있음.\n",
    "브런치 작가들이 글을 써서 브런치 매거진으로 보내고 매거진 에디터는 각 브런치 작가의 글 중 일부를 선별하여 승인절차를 통해 매거진을 구성할 수 있음.\n",
    "때문에 브런치 작가보다는 매거진의 영향이 더 크며 각 매거진이 하나의 독립 매체의 역할을 할 수 있음.\n",
    "즉 팀블로그로서 모집된 불특정 다수의 글에 선별과정을 거쳐 매거진이 만들어져 퀄리티가 높고 자연스럽게 SNS상에 퍼져나가며 영향력을 가짐.  \n",
    "\n",
    "* 사용자 식별자와 콘텐츠 식별자는 식별자 값에 '_' 존재 여부로 나뉨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data list\n",
    "- read.tar\n",
    "- metadata.json\n",
    "- /contents : data0~data6\n",
    "- users.json\n",
    "- magazine.json\n",
    "- predict.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read.tar (총 3,625개의 본 글 정보 파일) \n",
    "- 2018년 10월 1일부터 2019년 3월 1일까지 일부 브런치 독자들이 본 글의 정보\n",
    "- 파일 이름 : 시작일_종료일 ex) 2018110708_2018110709\n",
    "- 파일 구성 : 여러 줄로 구성, 하나의 줄은 독자가 파일의 시간 동안 본 글을 시간 순으로 기록한 것\n",
    "- 특정 글에 모바일, PC, 앱을 통해 접근함을 의미.\n",
    "- 머문 시간에 대한 정보가 제공되지 않기 때문에 글을 읽은 여부는 알 수 없음    \n",
    "\n",
    "ex ) file name : read/2019022823_2019030100  \n",
    "file contents : #8a706ac921a11004bab941d22323efab @bakchacruz_34 @wo-motivator_133 @wo-motivator_133  \n",
    "=> #8a706ac921a11004bab941d22323efab 라는 독자가 2019년 2월 28일 23시에서 2019년 3월 1일 0시 사이에\n",
    "   @bakchacruz_34 @wo-motivator_133 @wo-motivator_133 를 순서대로 보았다.\n",
    "\n",
    "#### metadata.json (643,104 줄로 구성된 글의 메타데이터)\n",
    "- 2018년 10월 1일부터 2019년 3월 14일까지 독자들이 본 글에 대한 정보\n",
    "- 비공개 전환, 삭제, 수정 등으로 유효하지 않거나 변동 될 수 있으며 본 글 정보에는 이 메타 데이터에 없는 글이 있을 수 있음\n",
    "- 개발 데이터와 평가 데이터에 포함된 글의 메타데이터도 포함 (모든 글에 대한 정보 포함)\n",
    "- 필드 설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|magazine_id| 이 글의 브런치 매거진 아이디 (없을 시는 0)|\n",
    "|reg_ts| 이 글이 등록된 시간(유닉스 시간, 밀리초)|\n",
    "|user_id| 작가 아이디|\n",
    "|article_id| 글 번호|\n",
    "|id| 글 식별자|\n",
    "|title| 제목|\n",
    "|sub_title| 부제목|\n",
    "|display_url| 웹 주소|\n",
    "|keyword_list| 작가가 부여한 글의 태그 정보|\n",
    "\n",
    "#### /contents (7개의 형태소 분석 결과 파일 **data.0 - data.6**로 구성)\n",
    "- 형태소 분석을 통해 추출된 정보를 암호화한 것\n",
    "- 카카오에서 공개한 형태소 분석기 \\textbf{khaii}의 기본 옵션 사용\n",
    "- 어휘 정보는 임의의 숫자로 1:1변환 (품사와 관계없이 동일 어휘는 같은 숫자로 변환)\n",
    "- HTML 등의 텍스트 제외 정보 잔류 가능 (전처리 필요)\n",
    "- 개발 데이터와 평가 데이터의 글 본문도 포함\n",
    "- 본문이 없는 글의 경우 제공되지 않음\n",
    "- 필드 설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|id| 글 식별자|\n",
    "|morphs | 형태소 분석 결과, 구분자 : \"/\"|    \n",
    "|chars | 형태소 분석 결과, 구분자 : \"+\"  \\\\  형태소 분석 결과에서 어휘 부분을 문자 단위로 암호화환 결과|\n",
    "\n",
    "\n",
    "ex ) \"안녕하세요 브런치입니다 \\n 안녕하세요\"   \n",
    "\n",
    "(morphs)$\\rightarrow$ [[\"8/NNG\", \"13/XSA\", \"81/EP\", \"888/EF\"], [\"0/NNP\", \"12913/VCP\", \"29/EC\"],[\"8/NNG\", \"13/XSA\", \"81/EP\", \"888/EF\"]]}  \n",
    "(chars)$\\rightarrow$ \"0+1+2/NNP\", \"4/VCP\", \"9+29+33/EC\"\n",
    "\n",
    "#### users.json (총 310,758명의 가입한 사용자 (작가 혹은 독자)의 정보)\n",
    "- 탈퇴 등의 이유로 사용자 정보가 없을 수 있음\n",
    "- 필드설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|keyword_list| 최근 며칠간 작가 글로 유입되었던 검색 키워드|\n",
    "|following_list| 구독 중인 작가 리스트|\n",
    "|id| 사용자 식별자|\n",
    "\n",
    "#### magazine.json (총 27,967개의 브런치 매거진 정보)\n",
    "- 필드 설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|id| 매거진 식별자|\n",
    "|magazine_tag_ |작가가 부여한 매거진의 태그 정보|\n",
    "\n",
    "#### predict.tar 디렉토리(예측할 사용자 정보)\n",
    "- dev.users: 개발 데이터. 예측한 성능을 평가하기 위해 제공한 사용자 3,000명 리스트\n",
    "- test.users: 평가 데이터. 최종 순위를 결정하기 위해 제공한 사용자 5,000명의 리스트\n",
    "- 일부 사용자는 2018년 10월 1일부터 2019년 3월 1일까지 본 글이 없을 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/Statlab2-pc1/Dropbox/MyPaper/data/brunch_data/utils\")\n",
    "\n",
    "from Blank import *\n",
    "from DataHD import *\n",
    "from TextTF import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import timedelta, datetime\n",
    "import glob\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_path = 'C:/Windows/Fonts/NanumGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "register_matplotlib_converters()\n",
    "\n",
    "directory = \"C:/Users/Statlab2-pc1/Dropbox/MyPaper/data/brunch_data\"\n",
    "\n",
    "\n",
    "with open(directory+\"/note/pickle/raw.pickle\", \"rb\") as f:\n",
    "    read_raw,contents,metadata,users,magazine,dev_users,test_users = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blank\n",
    "- DataHD\n",
    "- TextTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data\n",
    "data list\n",
    "- read.tar\n",
    "- metadata.json\n",
    "- /contents : data0~data6\n",
    "- users.json\n",
    "- magazine.json\n",
    "- predict.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file_lst = glob.glob(directory+\"/read/*\") # 특정한 패턴이나 확장자를 가진 파일들의 경로나 이름이 필요할 때 사용\n",
    "exclude_file_lst= ['read.tar']\n",
    "\n",
    "read_df_lst = []\n",
    "for f in read_file_lst:\n",
    "    file_name = os.path.basename(f) # path 내의 이름을 불러줌\n",
    "    if file_name in exclude_file_lst:\n",
    "        print(file_name)\n",
    "    else:\n",
    "        df_temp = pd.read_csv(f,header=None, names=['raw']) # colname = names['raw']\n",
    "        df_temp['dt'] = file_name[:8] # 8전까지 \n",
    "        df_temp['hr'] = file_name[8:10]\n",
    "        df_temp['user_id'] = df_temp['raw'].str.split(' ').str[0]\n",
    "        df_temp['article_id'] = df_temp['raw'].str.split(' ').str[1:].str.join(' ').str.strip()\n",
    "        read_df_lst.append(df_temp)\n",
    "read=pd.concat(read_df_lst)\n",
    "\n",
    "# \"read article count\" for each user \n",
    "read_cnt_by_user = read['article_id'].str.split(' ').map(len) # article_id ' '를 구분자로 개수 출력\n",
    "read_raw = pd.DataFrame({'dt' : np.repeat(read['dt'],read_cnt_by_user),\n",
    "                         'hr' : np.repeat(read['hr'],read_cnt_by_user),\n",
    "                         'user_id' : np.repeat(read['user_id'],read_cnt_by_user),\n",
    "                         'article_id' : dh.chainer(read['article_id'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 7/7 [1:07:42<00:00, 497.18s/it]\n"
     ]
    }
   ],
   "source": [
    "contents_file_list = glob.glob(directory+\"/contents/*\")\n",
    "\n",
    "delimiters = \"chars\", \"morphs\" ,\"id\"\n",
    "regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "\n",
    "contents_df_lst = []\n",
    "\n",
    "for f in tqdm(contents_file_list):\n",
    "    \n",
    "    file_name = os.path.basename(f)\n",
    "    \n",
    "    df_temp=[]\n",
    "    df_temp = pd.DataFrame(columns=['id','morphs','chars'])\n",
    "\n",
    "    for idx, line in enumerate(open(f)):\n",
    "\n",
    "        idd = re.split(regexPattern, line)[3][3:-3]\n",
    "        morphs = re.split(regexPattern, line)[2][3:-3]\n",
    "        chars = re.split(regexPattern, line)[1][3:-3]\n",
    "\n",
    "        df_temp.loc[idx] = [idd,morphs,chars]\n",
    "\n",
    "    contents_df_lst.append(df_temp)\n",
    "\n",
    "contents=pd.concat(contents_df_lst)\n",
    "#contents.to_csv(directory+\"/contents_total.csv\", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_json(directory+\"/metadata.json\",lines=True)\n",
    "users = pd.read_json(directory+\"/users.json\",lines=True)\n",
    "magazine = pd.read_json(directory+\"/magazine.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_file_list = glob.glob(directory+\"/predict/*\")\n",
    "    \n",
    "dev_users=df_temp = pd.read_csv(predict_file_list[0],header=None, names=['id'])\n",
    "test_users=df_temp = pd.read_csv(predict_file_list[1],header=None, names=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw.pickle\", \"wb\") as f:\n",
    "    pickle.dump((read_raw,contents,metadata,users,magazine,dev_users,test_users), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword 사용해서 ? -활용방안 모색\n",
    "### contents data \n",
    "- TM에 사용할 특정 형태소 추출하기\n",
    "- 품사 집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "4         True\n",
       "5         True\n",
       "6         True\n",
       "7         True\n",
       "8         True\n",
       "9         True\n",
       "10        True\n",
       "11        True\n",
       "12        True\n",
       "13        True\n",
       "14        True\n",
       "15        True\n",
       "16        True\n",
       "17        True\n",
       "18        True\n",
       "19        True\n",
       "20        True\n",
       "21        True\n",
       "22        True\n",
       "23        True\n",
       "24        True\n",
       "25        True\n",
       "26        True\n",
       "27        True\n",
       "28        True\n",
       "29        True\n",
       "         ...  \n",
       "42160     True\n",
       "42161     True\n",
       "42162     True\n",
       "42163     True\n",
       "42164     True\n",
       "42165     True\n",
       "42166     True\n",
       "42167     True\n",
       "42168     True\n",
       "42169     True\n",
       "42170     True\n",
       "42171     True\n",
       "42172     True\n",
       "42173     True\n",
       "42174     True\n",
       "42175     True\n",
       "42176     True\n",
       "42177     True\n",
       "42178     True\n",
       "42179     True\n",
       "42180     True\n",
       "42181     True\n",
       "42182     True\n",
       "42183     True\n",
       "42184     True\n",
       "42185     True\n",
       "42186     True\n",
       "42187     True\n",
       "42188    False\n",
       "42189     True\n",
       "Name: morphs, Length: 642190, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contents.to_csv(directory+\"/contents_total.csv\", mode='w')\n",
    "contents['morphs'].str.contains('NNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read.tar (총 3,625개의 본 글 정보 파일) \n",
    "- 2018년 10월 1일부터 2019년 3월 1일까지 일부 브런치 독자들이 본 글의 정보\n",
    "- 파일 이름 : 시작일_종료일 ex) 2018110708_2018110709\n",
    "- 파일 구성 : 여러 줄로 구성, 하나의 줄은 독자가 파일의 시간 동안 본 글을 시간 순으로 기록한 것\n",
    "- 특정 글에 모바일, PC, 앱을 통해 접근함을 의미.\n",
    "- 머문 시간에 대한 정보가 제공되지 않기 때문에 글을 읽은 여부는 알 수 없음    \n",
    "\n",
    "ex ) file name : read/2019022823_2019030100  \n",
    "file contents : #8a706ac921a11004bab941d22323efab @bakchacruz_34 @wo-motivator_133 @wo-motivator_133  \n",
    "=> #8a706ac921a11004bab941d22323efab 라는 독자가 2019년 2월 28일 23시에서 2019년 3월 1일 0시 사이에\n",
    "   @bakchacruz_34 @wo-motivator_133 @wo-motivator_133 를 순서대로 보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>hr</th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20181001</td>\n",
       "      <td>00</td>\n",
       "      <td>#e208be4ffea19b1ceb5cea2e3c4dc32c</td>\n",
       "      <td>@kty0613_91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20181001</td>\n",
       "      <td>00</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@miamiyoung_31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20181001</td>\n",
       "      <td>00</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@banksalad_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20181001</td>\n",
       "      <td>00</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@rlfrjsdn_95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20181001</td>\n",
       "      <td>00</td>\n",
       "      <td>#0a3d493f3b2318be80f391eaa00bfd1c</td>\n",
       "      <td>@readme999_140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dt  hr                            user_id      article_id\n",
       "0  20181001  00  #e208be4ffea19b1ceb5cea2e3c4dc32c     @kty0613_91\n",
       "1  20181001  00  #0a3d493f3b2318be80f391eaa00bfd1c  @miamiyoung_31\n",
       "1  20181001  00  #0a3d493f3b2318be80f391eaa00bfd1c   @banksalad_49\n",
       "1  20181001  00  #0a3d493f3b2318be80f391eaa00bfd1c    @rlfrjsdn_95\n",
       "1  20181001  00  #0a3d493f3b2318be80f391eaa00bfd1c  @readme999_140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metadata.json (643,104 줄로 구성된 글의 메타데이터)\n",
    "- 2018년 10월 1일부터 2019년 3월 14일까지 독자들이 본 글에 대한 정보\n",
    "- 비공개 전환, 삭제, 수정 등으로 유효하지 않거나 변동 될 수 있으며 본 글 정보에는 이 메타 데이터에 없는 글이 있을 수 있음\n",
    "- 개발 데이터와 평가 데이터에 포함된 글의 메타데이터도 포함 (모든 글에 대한 정보 포함)\n",
    "- 필드 설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|magazine_id| 이 글의 브런치 매거진 아이디 (없을 시는 0)|\n",
    "|reg_ts| 이 글이 등록된 시간(유닉스 시간, 밀리초)|\n",
    "|user_id| 작가 아이디|\n",
    "|article_id| 글 번호|\n",
    "|id| 글 식별자|\n",
    "|title| 제목|\n",
    "|sub_title| 부제목|\n",
    "|display_url| 웹 주소|\n",
    "|keyword_list| 작가가 부여한 글의 태그 정보|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>display_url</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword_list</th>\n",
       "      <th>magazine_id</th>\n",
       "      <th>reg_ts</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>782</td>\n",
       "      <td>https://brunch.co.kr/@bookdb/782</td>\n",
       "      <td>@bookdb_782</td>\n",
       "      <td>[여행, 호주, 국립공원]</td>\n",
       "      <td>8982</td>\n",
       "      <td>1474944427000</td>\n",
       "      <td>세상 어디에도 없는 호주 Top 10</td>\n",
       "      <td>사진으로 옮기기에도 아까운, 리치필드 국립공원</td>\n",
       "      <td>@bookdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>https://brunch.co.kr/@kohwang56/81</td>\n",
       "      <td>@kohwang56_81</td>\n",
       "      <td>[목련꽃, 아지랑이, 동행]</td>\n",
       "      <td>12081</td>\n",
       "      <td>1463092749000</td>\n",
       "      <td></td>\n",
       "      <td>[시] 서러운 봄</td>\n",
       "      <td>@kohwang56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>https://brunch.co.kr/@hannahajink/4</td>\n",
       "      <td>@hannahajink_4</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1447997287000</td>\n",
       "      <td>무엇 때문에</td>\n",
       "      <td>무엇을 위해</td>\n",
       "      <td>@hannahajink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "      <td>https://brunch.co.kr/@bryceandjuli/88</td>\n",
       "      <td>@bryceandjuli_88</td>\n",
       "      <td>[감정, 마음, 위로]</td>\n",
       "      <td>16315</td>\n",
       "      <td>1491055161000</td>\n",
       "      <td></td>\n",
       "      <td>싫다</td>\n",
       "      <td>@bryceandjuli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>https://brunch.co.kr/@mijeongpark/34</td>\n",
       "      <td>@mijeongpark_34</td>\n",
       "      <td>[유럽여행, 더블린, 아일랜드]</td>\n",
       "      <td>29363</td>\n",
       "      <td>1523292942000</td>\n",
       "      <td>#7. 내 친구의 집은 어디인가</td>\n",
       "      <td>Dubliner#7</td>\n",
       "      <td>@mijeongpark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                            display_url                id  \\\n",
       "0         782       https://brunch.co.kr/@bookdb/782       @bookdb_782   \n",
       "1          81     https://brunch.co.kr/@kohwang56/81     @kohwang56_81   \n",
       "2           4    https://brunch.co.kr/@hannahajink/4    @hannahajink_4   \n",
       "3          88  https://brunch.co.kr/@bryceandjuli/88  @bryceandjuli_88   \n",
       "4          34   https://brunch.co.kr/@mijeongpark/34   @mijeongpark_34   \n",
       "\n",
       "        keyword_list  magazine_id         reg_ts             sub_title  \\\n",
       "0     [여행, 호주, 국립공원]         8982  1474944427000  세상 어디에도 없는 호주 Top 10   \n",
       "1    [목련꽃, 아지랑이, 동행]        12081  1463092749000                         \n",
       "2                 []            0  1447997287000                무엇 때문에   \n",
       "3       [감정, 마음, 위로]        16315  1491055161000                         \n",
       "4  [유럽여행, 더블린, 아일랜드]        29363  1523292942000     #7. 내 친구의 집은 어디인가   \n",
       "\n",
       "                       title        user_id  \n",
       "0  사진으로 옮기기에도 아까운, 리치필드 국립공원        @bookdb  \n",
       "1                  [시] 서러운 봄     @kohwang56  \n",
       "2                     무엇을 위해   @hannahajink  \n",
       "3                         싫다  @bryceandjuli  \n",
       "4                 Dubliner#7   @mijeongpark  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### /contents (7개의 형태소 분석 결과 파일 **data.0 - data.6**로 구성)\n",
    "- 형태소 분석을 통해 추출된 정보를 암호화한 것\n",
    "- 카카오에서 공개한 형태소 분석기 \\textbf{khaii}의 기본 옵션 사용\n",
    "- 어휘 정보는 임의의 숫자로 1:1변환 (품사와 관계없이 동일 어휘는 같은 숫자로 변환)\n",
    "- HTML 등의 텍스트 제외 정보 잔류 가능 (전처리 필요)\n",
    "- 개발 데이터와 평가 데이터의 글 본문도 포함\n",
    "- 본문이 없는 글의 경우 제공되지 않음\n",
    "- 필드 설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|id| 글 식별자|\n",
    "|morphs | 형태소 분석 결과, 구분자 : \"/\"|    \n",
    "|chars | 형태소 분석 결과, 구분자 : \"+\"  \\\\  형태소 분석 결과에서 어휘 부분을 문자 단위로 암호화환 결과|\n",
    "\n",
    "\n",
    "ex ) \"안녕하세요 브런치입니다 \\n 안녕하세요\"   \n",
    "\n",
    "(morphs)$\\rightarrow$ [[\"8/NNG\", \"13/XSA\", \"81/EP\", \"888/EF\"], [\"0/NNP\", \"12913/VCP\", \"29/EC\"],[\"8/NNG\", \"13/XSA\", \"81/EP\", \"888/EF\"]]}  \n",
    "(chars)$\\rightarrow$ \"0+1+2/NNP\", \"4/VCP\", \"9+29+33/EC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>morphs</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@bookdb_782</td>\n",
       "      <td>[\"1\\/NNG\",\"2\\/JKB\"],[\"3\\/MAG\"],[\"4\\/VV\",\"5\\/ET...</td>\n",
       "      <td>[\"1+2\\/NNG\",\"3+4\\/JKB\"],[\"5+6\\/MAG\"],[\"7+8\\/VV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@kohwang56_81</td>\n",
       "      <td>[\"397\\/SW\",\"398\\/SW\"],[\"399\\/NNG\"],[\"175\\/VV\",...</td>\n",
       "      <td>[\"362\\/SW\",\"362+362\\/SW\"],[\"90+219+72\\/NNG\"],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@hannahajink_4</td>\n",
       "      <td>[\"180\\/NNG\",\"12\\/JKS\"],[\"483\\/VV\",\"37\\/EF\",\"14...</td>\n",
       "      <td>[\"140+153\\/NNG\",\"18\\/JKS\"],[\"400+115\\/VV\",\"44+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@bryceandjuli_88</td>\n",
       "      <td>[\"567\\/VA\",\"13\\/EF\",\"14\\/SF\"],[\"567\\/VA\",\"7\\/E...</td>\n",
       "      <td>[\"289\\/VA\",\"19\\/EF\",\"20\\/SF\"],[\"289\\/VA\",\"12\\/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mijeongpark_34</td>\n",
       "      <td>[\"667\\/NNG\"],[\"404\\/MM\",\"447\\/NNB\"],[\"668\\/NNB...</td>\n",
       "      <td>[\"42+77\\/NNG\"],[\"333\\/MM\",\"23\\/NNB\"],[\"388+478...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                             morphs  \\\n",
       "0       @bookdb_782  [\"1\\/NNG\",\"2\\/JKB\"],[\"3\\/MAG\"],[\"4\\/VV\",\"5\\/ET...   \n",
       "1     @kohwang56_81  [\"397\\/SW\",\"398\\/SW\"],[\"399\\/NNG\"],[\"175\\/VV\",...   \n",
       "2    @hannahajink_4  [\"180\\/NNG\",\"12\\/JKS\"],[\"483\\/VV\",\"37\\/EF\",\"14...   \n",
       "3  @bryceandjuli_88  [\"567\\/VA\",\"13\\/EF\",\"14\\/SF\"],[\"567\\/VA\",\"7\\/E...   \n",
       "4   @mijeongpark_34  [\"667\\/NNG\"],[\"404\\/MM\",\"447\\/NNB\"],[\"668\\/NNB...   \n",
       "\n",
       "                                               chars  \n",
       "0  [\"1+2\\/NNG\",\"3+4\\/JKB\"],[\"5+6\\/MAG\"],[\"7+8\\/VV...  \n",
       "1  [\"362\\/SW\",\"362+362\\/SW\"],[\"90+219+72\\/NNG\"],[...  \n",
       "2  [\"140+153\\/NNG\",\"18\\/JKS\"],[\"400+115\\/VV\",\"44+...  \n",
       "3  [\"289\\/VA\",\"19\\/EF\",\"20\\/SF\"],[\"289\\/VA\",\"12\\/...  \n",
       "4  [\"42+77\\/NNG\"],[\"333\\/MM\",\"23\\/NNB\"],[\"388+478...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### users.json (총 310,758명의 가입한 사용자 (작가 혹은 독자)의 정보)\n",
    "- 탈퇴 등의 이유로 사용자 정보가 없을 수 있음\n",
    "- 필드설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|keyword_list| 최근 며칠간 작가 글로 유입되었던 검색 키워드|\n",
    "|following_list| 구독 중인 작가 리스트|\n",
    "|id| 사용자 식별자|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of the subdataset is : 0.037148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@gabrieljmh',\n",
       " '@megaonic',\n",
       " '@cleancode',\n",
       " '@simu-look',\n",
       " '@daro',\n",
       " '@beba',\n",
       " '@2hananim',\n",
       " '@maama',\n",
       " '@rana-na',\n",
       " '@luna-moonkim',\n",
       " '@mongchi',\n",
       " '@stella-oh',\n",
       " '@yehyun86',\n",
       " '@kd4',\n",
       " '@hyorriet',\n",
       " '@naturally0318',\n",
       " '@yoyoyoyo',\n",
       " '@spoiledsalad',\n",
       " '@julieted17',\n",
       " '@giantbaby',\n",
       " '@junho85',\n",
       " '@dalkongmin',\n",
       " '@yamju',\n",
       " '@aerinitu',\n",
       " '@luciedream',\n",
       " '@dhong',\n",
       " '@eunbinbaek',\n",
       " '@authork',\n",
       " '@sweethard',\n",
       " '@sarry32',\n",
       " '@aria-grande',\n",
       " '@goodbyepain',\n",
       " '@brunchjqcb',\n",
       " '@travelpost',\n",
       " '@boldjournalcom',\n",
       " '@thankspizza',\n",
       " '@springchoi',\n",
       " '@bangcockbro',\n",
       " '@jsksoft',\n",
       " '@tamarorim',\n",
       " '@tryit',\n",
       " '@evergreenkyj',\n",
       " '@makersgonnamake',\n",
       " '@sadrove',\n",
       " '@borashow',\n",
       " '@eastrain',\n",
       " '@kimjiyoun',\n",
       " '@lklab2013',\n",
       " '@jade',\n",
       " '@zoiworld',\n",
       " '@pistol4747',\n",
       " '@breakthrough',\n",
       " '@skykamja24',\n",
       " '@yoonjikwon',\n",
       " '@artcom',\n",
       " '@hyejinchoi',\n",
       " '@anitooni',\n",
       " '@59hjhwc1d',\n",
       " '@jejutravellab',\n",
       " '@jaeseungmun',\n",
       " '@jidesign',\n",
       " '@lifidea',\n",
       " '@ebs',\n",
       " '@siliconroad',\n",
       " '@acgroup',\n",
       " '@sabumbyun',\n",
       " '@sustainlife',\n",
       " '@tuburkis',\n",
       " '@writingo119',\n",
       " '@ddny',\n",
       " '@emor',\n",
       " '@jejugrapher',\n",
       " '@techsuda',\n",
       " '@ssunfl',\n",
       " '@romanticgrey',\n",
       " '@sunnysideup',\n",
       " '@brunch']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nblist_user=Blank.listNBlank(users['keyword_list'])\n",
    "nblist_user.iloc[0]\n",
    "nblist_user['line'][1][1]['keyword']\n",
    "nblist_user['line'][1]\n",
    "Freq.prop(users,nblist_user)\n",
    "users['following_list'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### magazine.json (총 27,967개의 브런치 매거진 정보)\n",
    "- 필드 설명\n",
    "\n",
    "|변수명|변수 설명|\n",
    "|---|---|\n",
    "|id| 매거진 식별자|\n",
    "|magazine_tag_ |작가가 부여한 매거진의 태그 정보|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>magazine_tag_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38842</td>\n",
       "      <td>[브런치북, 육아일기, 대화법, 들려주고픈이야기]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11540</td>\n",
       "      <td>[tea, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11541</td>\n",
       "      <td>[food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11546</td>\n",
       "      <td>[브런치북, 일상, 시, 사람]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11544</td>\n",
       "      <td>[감성에세이, 노래, 음악에세이]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            magazine_tag_list\n",
       "0  38842  [브런치북, 육아일기, 대화법, 들려주고픈이야기]\n",
       "1  11540                  [tea, food]\n",
       "2  11541                       [food]\n",
       "3  11546            [브런치북, 일상, 시, 사람]\n",
       "4  11544           [감성에세이, 노래, 음악에세이]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magazine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 매거진 태그 리스트를 DTM 형식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    " \n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    " \n",
    "# tokenizer : 문장에서 색인어 추출을 위해 명사,동사,알파벳,숫자 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"], stopword=[]):\n",
    "    return [\n",
    "        word for word, tag in twitter.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅋ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 리스트로 묶음\n",
    "mag_tag_list=TextTF.toText(magazine['magazine_tag_list'])[:1000]\n",
    "type(mag_tag_list['tag_text'])\n",
    "\n",
    "mag_tag_list\n",
    "vectorize = CountVectorizer(tokenizer=tokenizer)\n",
    "text = vectorize.fit_transform(mag_tag_list['tag_text'])\n",
    "magtag_words = vectorize.get_feature_names() # 문장에서 뽑아낸 feature 들의 배열 : term\n",
    "magtag_dtm=pd.DataFrame(text.A[0:,0:],\n",
    "                        columns=[magtag_words[idx] for idx in range(text.shape[1])])\n",
    "magtag_list=magazine['magazine_tag_list'].tolist()\n",
    "\n",
    "with open(directory+\"/note/pickle/magtag.pickle\", \"wb\") as f:\n",
    "    pickle.dump((magtag_dtm,magtag_words,magtag_list),f)\n",
    "    \n",
    "# dtm_magtag.sum(axis=1) # observation wise\n",
    "# words\n",
    "magtag_dtm.to_csv(directory+\"/note/D_scribble/magtag_dtm.csv\", mode='w', encoding='utf-8')\n",
    "\n",
    "with open(directory+\"/note/D_scribble/magtag_words.txt\",'w', encoding='utf-8') as f:\n",
    "    for item in magtag_words:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict.tar 디렉토리(예측할 사용자 정보)\n",
    "- dev.users: 개발 데이터. 예측한 성능을 평가하기 위해 제공한 사용자 3,000명 리스트\n",
    "- test.users: 평가 데이터. 최종 순위를 결정하기 위해 제공한 사용자 5,000명의 리스트\n",
    "- 일부 사용자는 2018년 10월 1일부터 2019년 3월 1일까지 본 글이 없을 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e014d12f8178>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdev_users\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_users\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dev_users' is not defined"
     ]
    }
   ],
   "source": [
    "dev_users.head()\n",
    "test_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ariticle= pd.merge(read_raw, \n",
    "                       metadata,\n",
    "                       left_on='article_id',\n",
    "                       right_on='id',\n",
    "                       how=\"inner\")\n",
    "\n",
    "print(\"[article_id] time : \",read_raw.shape)\n",
    "print(\"[id] (sub) title : \",metadata.shape)\n",
    "print(\"With overlap : \",raw_ariticle.shape)\n",
    "print('--------------------------------------------------')\n",
    "print(list(raw_ariticle.columns))\n",
    "raw_ariticle.shape[0]==sum(raw_ariticle['article_id_x']== raw_ariticle['id']) # verify\n",
    "raw_ariticle=raw_ariticle.drop(columns='article_id_x')\n",
    "\n",
    "with open(directory+\"/note/pickle/raw_ariticle.pickle\", \"wb\") as f:\n",
    "    pickle.dump(raw_ariticle, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
